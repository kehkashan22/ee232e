{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -2.7956e+04  3e+04  2e-02  9e+00  1e+00\n",
      " 1:  1.5533e+02 -1.9562e+04  2e+04  2e-02  6e+00  2e+00\n",
      " 2:  3.8213e+02 -1.5725e+04  2e+04  1e-02  5e+00  3e+00\n",
      " 3:  5.5219e+02 -8.0427e+03  1e+04  7e-03  3e+00  3e+00\n",
      " 4:  2.2699e+02 -6.5420e+02  1e+03  7e-04  3e-01  3e-01\n",
      " 5:  2.9461e+01 -3.4959e+01  7e+01  5e-05  2e-02  3e-02\n",
      " 6:  3.1613e-01 -3.4717e-01  7e-01  5e-07  2e-04  4e-04\n",
      " 7:  3.1610e-03 -3.4712e-03  7e-03  5e-09  2e-06  4e-06\n",
      " 8:  3.1610e-05 -3.4712e-05  7e-05  5e-11  2e-08  4e-08\n",
      " 9:  3.1610e-07 -3.4712e-07  7e-07  5e-13  2e-10  4e-10\n",
      "10:  3.1610e-09 -3.4712e-09  7e-09  5e-15  2e-12  4e-12\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "R = irl(100, env.PU, env.PD, env.PL, env.PR , 2, 0.8, 100, P2)\n",
    "estimate_reward = np.asarray(R).reshape(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def heap_map(reward):\n",
    "    x = np.arange(11)\n",
    "    y = np.arange(11)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    plt.pcolor(X, Y, reward, edgecolors='k', linewidths=0.5 )\n",
    "    plt.colorbar()\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "reward_2 = np.zeros((10,10))\n",
    "reward_2[1,4:7] = -100\n",
    "reward_2[2:7,4] = -100\n",
    "reward_2[2:4,6] = -100\n",
    "reward_2[3,7] = -100\n",
    "reward_2[3:8,8] = -100\n",
    "reward_2[7,6:8] = -100\n",
    "reward_2[8,6] = -100\n",
    "reward_2[-1,-1] = 10\n",
    "plt.figure()\n",
    "heap_map(reward_2)\n",
    "plt.figure()\n",
    "heap_map(estimate_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "value_iteration() missing 1 required positional argument: 'R'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d95d1cf710da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheap_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: value_iteration() missing 1 required positional argument: 'R'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "heap_map(value_iteration(Environment(w=0.1,disc=0.8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "optimal_policy() missing 1 required positional argument: 'R'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cea7ab3d16dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#action 0 is 'up'; action 1 is 'down'; action 2 is 'left'; action 3 is 'right'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marrow_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'↑'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'↓'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'←'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'→'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0marrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marrow_visual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'↑'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: optimal_policy() missing 1 required positional argument: 'R'"
     ]
    }
   ],
   "source": [
    "#action 0 is 'up'; action 1 is 'down'; action 2 is 'left'; action 3 is 'right'\n",
    "arrow_dict = {0:'↑', 1:'↓', 2:'←', 3:'→'}\n",
    "arrow = optimal_policy(Environment(w=0.1,disc=0.8))\n",
    "arrow = np.asarray(arrow)\n",
    "arrow_visual = [['↑' for x in range(10)] for y in range(10)]\n",
    "\n",
    "for i in range(arrow.shape[0]):\n",
    "    for j in range(arrow.shape[1]):\n",
    "        arrow_visual[i][j] = arrow_dict[arrow[i][j]]\n",
    "\n",
    "plt.figure()\n",
    "tb = plt.table(cellText=arrow_visual, loc=(0,0))\n",
    "tc = tb.properties()['child_artists']\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "import random\n",
    "from numpy.linalg import inv\n",
    "\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create MDP environment\n",
    "class Environment():\n",
    "    \"\"\"\n",
    "    members: - state set S, \n",
    "             - action set A, \n",
    "             - w, discount factor, \n",
    "              - reward function\n",
    "             - transition probability matrix PU(100*100), PD(100*100), PL(100*100), PR(100*100).\n",
    "    \"\"\"    \n",
    "    def __init__(self, w, disc):\n",
    "        self.w = w\n",
    "        self.disc = disc\n",
    "        #state set\n",
    "        self.S = numpy.zeros(shape=(10,10))\n",
    "        self.S = [[(0.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(1.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(2.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(3.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(4.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(5.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(6.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(7.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(8.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(9.0 + 10.0 * i) for i in range(10)]]\n",
    "\n",
    "        #action set\n",
    "        self.A = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "        #Transition probability\n",
    "        PU = numpy.zeros(shape=(100,100))#up transition probability\n",
    "        PD = numpy.zeros(shape=(100,100))#down transition probability\n",
    "        PL = numpy.zeros(shape=(100,100))#left transition probability\n",
    "        PR = numpy.zeros(shape=(100,100))#right transition probability\n",
    "        \n",
    "        #row is state at time step t, column is state at time step t+1\n",
    "        # for agent at edge states\n",
    "        for i in range(1,9):\n",
    "            PU[i][i] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i-1] = self.w/4.0 #<\n",
    "            PU[i][i+1] = self.w/4.0 #>\n",
    "            PU[i][i+10] = self.w/4.0 #!\n",
    "            PD[i][i] = self.w/4.0\n",
    "            PD[i][i-1] = self.w/4.0\n",
    "            PD[i][i+1] = self.w/4.0\n",
    "            PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i] = self.w/4.0\n",
    "            PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i+1] = self.w/4.0\n",
    "            PL[i][i+10] = self.w/4.0\n",
    "            PR[i][i] = self.w/4.0\n",
    "            PR[i][i-1] = self.w/4.0\n",
    "            PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i+10] = self.w/4.0\n",
    "        for j in range(1,9):\n",
    "            i = 10*j\n",
    "            PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i] = self.w/4.0 #<\n",
    "            PU[i][i+1] = self.w/4.0 #>\n",
    "            PU[i][i+10] = self.w/4.0 #!\n",
    "            PD[i][i-10] = self.w/4.0\n",
    "            PD[i][i] = self.w/4.0\n",
    "            PD[i][i+1] = self.w/4.0\n",
    "            PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i-10] = self.w/4.0\n",
    "            PL[i][i] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i+1] = self.w/4.0\n",
    "            PL[i][i+10] = self.w/4.0\n",
    "            PR[i][i-10] = self.w/4.0\n",
    "            PR[i][i] = self.w/4.0\n",
    "            PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i+10] = self.w/4.0\n",
    "        for k in range(1,9):\n",
    "            i = 9 + k*10\n",
    "            PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i-1] = self.w/4.0 #<\n",
    "            PU[i][i] = self.w/4.0 #>\n",
    "            PU[i][i+10] = self.w/4.0 #!\n",
    "            PD[i][i-10] = self.w/4.0\n",
    "            PD[i][i-1] = self.w/4.0\n",
    "            PD[i][i] = self.w/4.0\n",
    "            PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i-10] = self.w/4.0\n",
    "            PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i] = self.w/4.0\n",
    "            PL[i][i+10] = self.w/4.0\n",
    "            PR[i][i-10] = self.w/4.0\n",
    "            PR[i][i-1] = self.w/4.0\n",
    "            PR[i][i] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i+10] = self.w/4.0\n",
    "        for n in range(1,9):\n",
    "            i = 90 + n\n",
    "            PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i-1] = self.w/4.0 #<\n",
    "            PU[i][i+1] = self.w/4.0 #>\n",
    "            PU[i][i] = self.w/4.0 #!\n",
    "            PD[i][i-10] = self.w/4.0\n",
    "            PD[i][i-1] = self.w/4.0\n",
    "            PD[i][i+1] = self.w/4.0\n",
    "            PD[i][i] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i-10] = self.w/4.0\n",
    "            PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i+1] = self.w/4.0\n",
    "            PL[i][i] = self.w/4.0\n",
    "            PR[i][i-10] = self.w/4.0\n",
    "            PR[i][i-1] = self.w/4.0\n",
    "            PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i] = self.w/4.0\n",
    "        \n",
    "            \n",
    "        # for agent at corner states\n",
    "        PU[0][0] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PU[0][1] = self.w/4.0\n",
    "        PU[0][10] = self.w/4.0\n",
    "        PD[0][0] = self.w/4.0 + self.w/4.0\n",
    "        PD[0][1] = self.w/4.0\n",
    "        PD[0][10] = 1 - self.w + self.w/4.0 \n",
    "        PL[0][0] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PL[0][1] = self.w/4.0\n",
    "        PL[0][10] = self.w/4.0\n",
    "        PR[0][0] = self.w/4.0 + self.w/4.0\n",
    "        PR[0][1] = 1 - self.w + self.w/4.0\n",
    "        PR[0][10] = self.w/4.0\n",
    "                \n",
    "        PU[9][9] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PU[9][8] = self.w/4.0\n",
    "        PU[9][19] = self.w/4.0        \n",
    "        PD[9][9] = self.w/4.0 + self.w/4.0\n",
    "        PD[9][8] = self.w/4.0\n",
    "        PD[9][19] = 1 - self.w + self.w/4.0\n",
    "        PL[9][9] = self.w/4.0 + self.w/4.0\n",
    "        PL[9][8] = 1 - self.w + self.w/4.0 \n",
    "        PL[9][19] = self.w/4.0\n",
    "        PR[9][9] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PR[9][8]= self.w/4.0\n",
    "        PR[9][19] = self.w/4.0\n",
    "           \n",
    "        PU[90][90] = self.w/4.0 + self.w/4.0\n",
    "        PU[90][80] = 1 - self.w + self.w/4.0 \n",
    "        PU[90][91] = self.w/4.0            \n",
    "        PD[90][90] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PD[90][80] = self.w/4.0\n",
    "        PD[90][91] = self.w/4.0           \n",
    "        PL[90][90] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PL[90][80] = self.w/4.0\n",
    "        PL[90][91] = self.w/4.0           \n",
    "        PR[90][90] = self.w/4.0 + self.w/4.0\n",
    "        PR[90][80]= self.w/4.0\n",
    "        PR[90][91] = 1 - self.w + self.w/4.0 \n",
    "        \n",
    "        PU[99][99] = self.w/4.0 + self.w/4.0\n",
    "        PU[99][89] = 1 - self.w + self.w/4.0 \n",
    "        PU[99][98] = self.w/4.0         \n",
    "        PD[99][99] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PD[99][89] = self.w/4.0\n",
    "        PD[99][98] = self.w/4.0           \n",
    "        PL[99][99] = self.w/4.0 + self.w/4.0  \n",
    "        PL[99][89] = self.w/4.0\n",
    "        PL[99][98] = 1 - self.w + self.w/4.0          \n",
    "        PR[99][99] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PR[99][89]= self.w/4.0\n",
    "        PR[99][98] = self.w/4.0\n",
    "        \n",
    "        # for other states\n",
    "        for m in range(1,9):\n",
    "            for n in range(1,9):\n",
    "                i = m*10 + n\n",
    "                PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "                PU[i][i-1] = self.w/4.0 #<\n",
    "                PU[i][i+1] = self.w/4.0 #>\n",
    "                PU[i][i+10] = self.w/4.0 #!\n",
    "                PD[i][i-10] = self.w/4.0\n",
    "                PD[i][i-1] = self.w/4.0\n",
    "                PD[i][i+1] = self.w/4.0\n",
    "                PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "                PL[i][i-10] = self.w/4.0\n",
    "                PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "                PL[i][i+1] = self.w/4.0\n",
    "                PL[i][i+10] = self.w/4.0\n",
    "                PR[i][i-10] = self.w/4.0\n",
    "                PR[i][i-1] = self.w/4.0\n",
    "                PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "                PR[i][i+10] = self.w/4.0\n",
    "        \n",
    "        self.PU = PU\n",
    "        self.PD = PD\n",
    "        self.PL = PL\n",
    "        self.PR = PR\n",
    "        \n",
    "        #reward function\n",
    "        \n",
    "        reward = [0]*100\n",
    "        reward[14] = -100\n",
    "        reward[15] = -100\n",
    "        reward[16] = -100\n",
    "        reward[24] = -100\n",
    "        reward[26] = -100\n",
    "        reward[34] = -100\n",
    "        reward[36] = -100\n",
    "        reward[37] = -100\n",
    "        reward[38] = -100\n",
    "        reward[44] = -100\n",
    "        reward[48] = -100\n",
    "        reward[54] = -100\n",
    "        reward[58] = -100\n",
    "        reward[64] = -100\n",
    "        reward[68] = -100\n",
    "        reward[76] = -100\n",
    "        reward[77] = -100\n",
    "        reward[78] = -100\n",
    "        reward[86] = -100\n",
    "        reward[-1] = 10\n",
    "        \"\"\"\n",
    "        reward = [0]*100\n",
    "        reward[99] = 1.0\n",
    "        \"\"\"\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(w =0.1,disc = gamma) \n",
    "\n",
    "# action 0 is 'up'; action 1 is 'down'; action 2 is 'left'; action 3 is 'right'.\n",
    "\n",
    "P2 = [[1, 1, 1, 2, 2, 3, 3, 3, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 0, 3, 3, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 3, 3, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 1, 0, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 1, 1, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 1, 2, 3, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 2, 2, 3, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 2, 1, 1, 1],\n",
    "      [3, 3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [3, 3, 3, 3, 3, 3, 3, 3, 3, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:232: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    " # optimal function# optima \n",
    "def value_iteration(env,R):\n",
    "    #initialization\n",
    "    V = env.S.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            V[i][j] = 0\n",
    "    change = math.inf\n",
    "    \n",
    "    \n",
    "    #estimation\n",
    "    while(change > 0.01):\n",
    "        change = 0\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                v = V[i][j]\n",
    "                k = i * 10 + j\n",
    "                sumu = 0\n",
    "                sumd = 0\n",
    "                suml = 0\n",
    "                sumr = 0\n",
    "                c = 0\n",
    "                for m in env.PU[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PD[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PL[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PR[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                V[i][j] = max(sumd, sumu, sumr, suml)\n",
    "\n",
    "                change = max(change, abs(v - V[i][j]))\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal function# optima \n",
    "def value_iteration_modification(env,R,epi):\n",
    "    #initialization\n",
    "    V = env.S.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            V[i][j] = 0\n",
    "    change = math.inf\n",
    "    \n",
    "    \n",
    "    #estimation\n",
    "    while(change > 0.01):\n",
    "        change = 0\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                v = V[i][j]\n",
    "                k = i * 10 + j\n",
    "                sumu = 0\n",
    "                sumd = 0\n",
    "                suml = 0\n",
    "                sumr = 0\n",
    "                c = 0\n",
    "                for m in env.PU[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PD[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PL[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PR[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                rnd = random.random() \n",
    "                if rnd < epi: \n",
    "                    V[i][j] = max(sumd, sumu, sumr, suml)\n",
    "                else:\n",
    "                    action = [0,1,2,3]\n",
    "                    values = [sumd, sumu, sumr, suml]\n",
    "                    action_taken = random.sample(action,1)\n",
    "                    #print(action_taken)\n",
    "                    V[i][j] = values[action_taken[0]]\n",
    "                change = max(change, abs(v - V[i][j]))\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_border(state, action):\n",
    "    if (state%10 == 0):\n",
    "        return (action == 2)\n",
    "    elif (state%10 == 9):\n",
    "        return (action == 3)\n",
    "    elif (state >= 0 and state <= 9):\n",
    "        return (action == 0)\n",
    "    elif (state < 99 and state >= 90):\n",
    "        return (action == 1)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy_m(env,R,V):\n",
    "    arrow=[[0 for x in range(10)] for y in range(10)]\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            v = V[i][j]\n",
    "            k = i * 10 + j\n",
    "            sumu = 0\n",
    "            sumd = 0\n",
    "            suml = 0\n",
    "            sumr = 0\n",
    "            c = 0\n",
    "            maxnum =0\n",
    "            for m in env.PU[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            #maxnum=sumu\n",
    "            c = 0\n",
    "            for m in env.PD[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            \n",
    "            c = 0\n",
    "            for m in env.PL[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            c = 0\n",
    "            \n",
    "            for m in env.PR[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "\n",
    "            action_values = [sumu,sumd,suml,sumr]\n",
    "            action = np.argmax(action_values)\n",
    "            if out_of_border(k,action):\n",
    "                action_values[action] = -9999\n",
    "                action = np.argmax(action_values)\n",
    "            arrow[i][j] = action\n",
    "            \n",
    "    #print(arrow)\n",
    "    return arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(env,R):\n",
    "    V=value_iteration(env,R)\n",
    "    arrow=[[0 for x in range(10)] for y in range(10)]\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            v = V[i][j]\n",
    "            k = i * 10 + j\n",
    "            sumu = 0\n",
    "            sumd = 0\n",
    "            suml = 0\n",
    "            sumr = 0\n",
    "            c = 0\n",
    "            maxnum =0\n",
    "            for m in env.PU[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            maxnum=sumu\n",
    "            c = 0\n",
    "            for m in env.PD[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            if(sumd>maxnum):\n",
    "                arrow[i][j]=1\n",
    "                maxnum=sumd\n",
    "            \n",
    "            c = 0\n",
    "            for m in env.PL[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            c = 0\n",
    "            if(suml>maxnum):\n",
    "                arrow[i][j]=2\n",
    "                maxnum=suml\n",
    "            \n",
    "            for m in env.PR[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            if(sumr>maxnum):\n",
    "                arrow[i][j]=3\n",
    "                maxnum=sumr \n",
    "    #print(arrow)\n",
    "    return arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "def irl(state_num, PU, PD, PL, PR , lambda_num, discount, Rmax, policy):\n",
    "    \"\"\"\n",
    "    - state_num: number of state = 100;\n",
    "    - PU, PD, PL, PR: four transition probability matrix 100*100;\n",
    "    - lambda_num: adjustable pernalty coefficient;\n",
    "    - discount: disc;\n",
    "    - Rmax: max value in T_rewardf;\n",
    "    - policy: policy function 10*10, values should be in [0, 1, 2, 3];\n",
    "    \n",
    "     should return the estimated reward function of ground truth reward function 1 or 2.\n",
    "    \n",
    "    \"\"\"\n",
    "    # change reward function dimension to 100*1\n",
    "    policy = np.reshape(policy,(100,1))\n",
    "    #print(policy[1])\n",
    "    \n",
    "    #action set\n",
    "    A = [0, 1, 2, 3]\n",
    "    \n",
    "    # transfer four transition probability matrix to a three dimension probability tensor \n",
    "    P = [[[0]*100]*100]*4\n",
    "    P[0] = PU\n",
    "    P[1] = PD\n",
    "    P[2] = PL\n",
    "    P[3] = PR\n",
    "    #P = np.asarray(P)\n",
    "    #print(P[3][5])\n",
    "    \n",
    "    # a handle for numtiplication in P.\n",
    "    def multiply(state, other_action):\n",
    "        #print(state)\n",
    "        #policy_action = policy[state][0]\n",
    "        policy_action = int(policy[state])\n",
    "        mul = - np.dot(P[policy_action][state] - P[other_action][state]\n",
    "        ,inv(np.eye(state_num) - discount * P[policy_action]))\n",
    "        #print(mul)\n",
    "        return mul\n",
    "    \n",
    "    # D inclusing following parts: [D00 D01 D02]\n",
    "    #                              [D10 D11 D12]\n",
    "    #                              [D20 D21 D22]\n",
    "    #                              [D30 D31 D32]\n",
    "    #                              [D40 D41 D42]\n",
    "    #                              [D50 D51 D52]\n",
    "    \n",
    "    #D00 (300*100)\n",
    "    D00 = np.vstack([multiply(s, a)] for s in range(100) for a in [element for element in A if element!=policy[s][0]])\n",
    "    D01 = np.vstack([np.eye(1, 100, s) for s in range(100) for a in [element for element in A if element!=policy[s][0]]])\n",
    "    D02 = np.zeros((300, 100))\n",
    "    #D03 = np.zeros((300, 100))\n",
    "    \n",
    "    D0 = np.hstack((D00, D01, D02))\n",
    "    \n",
    "    \n",
    "    D10 = np.vstack([multiply(s, a)] for s in range(100) for a in [element for element in A if element!=policy[s][0]])    \n",
    "    D11 = np.zeros((300, 100))\n",
    "    D12 = np.zeros((300, 100))\n",
    "    #D13 = np.zeros((300, 100))\n",
    "    \n",
    "    D1 = np.hstack((D10, D11, D12))\n",
    "    \n",
    "    D20 = np.eye(100 , 100)\n",
    "    D21 = np.zeros((100, 100))\n",
    "    D22 = - np.eye(100 , 100)\n",
    "    #D23 = np.zeros((100, 100))\n",
    "      \n",
    "    D2 = np.hstack((D20, D21, D22))\n",
    "    \n",
    "    D30 = - np.eye(100 , 100)\n",
    "    D31 = np.zeros((100, 100))\n",
    "    D32 = - np.eye(100 , 100)\n",
    "    #D33 = np.zeros((100, 100))\n",
    "      \n",
    "    D3 = np.hstack((D30, D31, D32))\n",
    "    \n",
    "    D40 = np.eye(100 , 100)\n",
    "    D41 = np.zeros((100, 100))\n",
    "    D42 = np.zeros((100, 100))\n",
    "    #D43 = - Rmax * np.eye(100 , 100)\n",
    "      \n",
    "    D4 = np.hstack((D40, D41, D42))\n",
    "        \n",
    "    D50 = - np.eye(100 , 100)\n",
    "    D51 = np.zeros((100, 100))\n",
    "    D52 = np.zeros((100, 100))\n",
    "    #D53 = - Rmax * np.eye(100 , 100)\n",
    "      \n",
    "    D5 = np.hstack((D50, D51, D52))  \n",
    "        \n",
    "    D = np.vstack((D0, D1, D2, D3, D4, D5))\n",
    "    #print(D.shape)\n",
    "    CT = -np.hstack((np.zeros(100), np.ones(100), -lambda_num * np.ones(100)))\n",
    "    \n",
    "    b = np.zeros((800, 1))\n",
    "    bmax = Rmax * np.ones(200)\n",
    "    b = np.append(b, bmax)\n",
    "    #print(b.shape)\n",
    "    \n",
    "    D = matrix(D)\n",
    "    CT = matrix(CT)\n",
    "    b = matrix(b)\n",
    "    sol=solvers.lp(CT,D,b)\n",
    "    r = np.asarray(sol[\"x\"][:100], dtype=np.double)\n",
    "    #print(len(sol['x']))\n",
    "    return r.reshape((100,))\n",
    "    #return sol['x'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -2.7956e+04  3e+04  2e-02  2e+01  1e+00\n",
      " 1:  2.3437e+01 -2.1139e+04  2e+04  2e-02  2e+01  2e+00\n",
      " 2:  3.9067e+01 -1.5687e+04  2e+04  1e-02  1e+01  3e+00\n",
      " 3: -4.5156e+01 -7.1241e+03  9e+03  6e-03  5e+00  3e+00\n",
      " 4: -3.8990e+02 -4.0974e+03  5e+03  3e-03  3e+00  3e+00\n",
      " 5: -1.6667e+03 -5.6357e+03  9e+03  3e-03  3e+00  5e+00\n",
      " 6: -2.1817e+03 -6.1237e+03  1e+04  3e-03  3e+00  6e+00\n",
      " 7: -3.2758e+03 -5.7861e+03  8e+03  2e-03  2e+00  5e+00\n",
      " 8: -4.2184e+03 -5.7268e+03  6e+03  1e-03  1e+00  4e+00\n",
      " 9: -4.2513e+03 -5.5006e+03  5e+03  1e-03  9e-01  4e+00\n",
      "10: -4.9769e+03 -5.4492e+03  2e+03  4e-04  3e-01  2e+00\n",
      "11: -5.1688e+03 -5.4469e+03  1e+03  2e-04  2e-01  1e+00\n",
      "12: -5.2332e+03 -5.4506e+03  1e+03  2e-04  2e-01  9e-01\n",
      "13: -5.3787e+03 -5.4837e+03  5e+02  8e-05  8e-02  5e-01\n",
      "14: -5.3813e+03 -5.4815e+03  5e+02  8e-05  7e-02  4e-01\n",
      "15: -5.4715e+03 -5.5071e+03  2e+02  3e-05  3e-02  2e-01\n",
      "16: -5.5074e+03 -5.5165e+03  5e+01  7e-06  7e-03  4e-02\n",
      "17: -5.5102e+03 -5.5167e+03  3e+01  5e-06  5e-03  3e-02\n",
      "18: -5.5169e+03 -5.5185e+03  8e+00  1e-06  1e-03  8e-03\n",
      "19: -5.5184e+03 -5.5189e+03  2e+00  4e-07  3e-04  2e-03\n",
      "20: -5.5189e+03 -5.5190e+03  4e-01  6e-08  6e-05  4e-04\n",
      "21: -5.5190e+03 -5.5190e+03  5e-02  8e-09  7e-06  5e-05\n",
      "22: -5.5190e+03 -5.5190e+03  6e-04  1e-10  9e-08  6e-07\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimal_policy() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f8bbb213d421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mirl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPR\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0marrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: optimal_policy() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def s_range(start, end, step):\n",
    "    while start <= end:\n",
    "        yield start\n",
    "        start += step\n",
    "\n",
    "accuracy = []\n",
    "for i in s_range(0, 5, 0.01):\n",
    "    print(i)\n",
    "    R = irl(100, env.PU, env.PD, env.PL, env.PR , i, gamma, 100, P2) \n",
    "    arrow = optimal_policy(Environment(w=0.1,disc=gamma),R,0)\n",
    "    P = np.asarray(P2)\n",
    "    acc = np.sum(arrow == P)\n",
    "    accuracy = np.append(accuracy, acc/100)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying function\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "import random\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \"\"\"\n",
    "    members: - state set S, \n",
    "             - action set A, \n",
    "             - w, discount factor, \n",
    "              - reward function\n",
    "             - transition probability matrix PU(100*100), PD(100*100), PL(100*100), PR(100*100).\n",
    "    \"\"\"    \n",
    "    def __init__(self, w, disc):\n",
    "        self.w = w\n",
    "        self.disc = disc\n",
    "        #state set\n",
    "        self.S = numpy.zeros(shape=(10,10))\n",
    "        self.S = [[(0.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(1.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(2.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(3.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(4.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(5.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(6.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(7.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(8.0 + 10.0 * i) for i in range(10)],\n",
    "                  [(9.0 + 10.0 * i) for i in range(10)]]\n",
    "\n",
    "        #action set\n",
    "        self.A = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "        #Transition probability\n",
    "        PU = numpy.zeros(shape=(100,100))#up transition probability\n",
    "        PD = numpy.zeros(shape=(100,100))#down transition probability\n",
    "        PL = numpy.zeros(shape=(100,100))#left transition probability\n",
    "        PR = numpy.zeros(shape=(100,100))#right transition probability\n",
    "        \n",
    "        #row is state at time step t, column is state at time step t+1\n",
    "        # for agent at edge states\n",
    "        for i in range(1,9):\n",
    "            PU[i][i] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i-1] = self.w/4.0 #<\n",
    "            PU[i][i+1] = self.w/4.0 #>\n",
    "            PU[i][i+10] = self.w/4.0 #!\n",
    "            PD[i][i] = self.w/4.0\n",
    "            PD[i][i-1] = self.w/4.0\n",
    "            PD[i][i+1] = self.w/4.0\n",
    "            PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i] = self.w/4.0\n",
    "            PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i+1] = self.w/4.0\n",
    "            PL[i][i+10] = self.w/4.0\n",
    "            PR[i][i] = self.w/4.0\n",
    "            PR[i][i-1] = self.w/4.0\n",
    "            PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i+10] = self.w/4.0\n",
    "        for j in range(1,9):\n",
    "            i = 10*j\n",
    "            PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i] = self.w/4.0 #<\n",
    "            PU[i][i+1] = self.w/4.0 #>\n",
    "            PU[i][i+10] = self.w/4.0 #!\n",
    "            PD[i][i-10] = self.w/4.0\n",
    "            PD[i][i] = self.w/4.0\n",
    "            PD[i][i+1] = self.w/4.0\n",
    "            PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i-10] = self.w/4.0\n",
    "            PL[i][i] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i+1] = self.w/4.0\n",
    "            PL[i][i+10] = self.w/4.0\n",
    "            PR[i][i-10] = self.w/4.0\n",
    "            PR[i][i] = self.w/4.0\n",
    "            PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i+10] = self.w/4.0\n",
    "        for k in range(1,9):\n",
    "            i = 9 + k*10\n",
    "            PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i-1] = self.w/4.0 #<\n",
    "            PU[i][i] = self.w/4.0 #>\n",
    "            PU[i][i+10] = self.w/4.0 #!\n",
    "            PD[i][i-10] = self.w/4.0\n",
    "            PD[i][i-1] = self.w/4.0\n",
    "            PD[i][i] = self.w/4.0\n",
    "            PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i-10] = self.w/4.0\n",
    "            PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i] = self.w/4.0\n",
    "            PL[i][i+10] = self.w/4.0\n",
    "            PR[i][i-10] = self.w/4.0\n",
    "            PR[i][i-1] = self.w/4.0\n",
    "            PR[i][i] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i+10] = self.w/4.0\n",
    "        for n in range(1,9):\n",
    "            i = 90 + n\n",
    "            PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "            PU[i][i-1] = self.w/4.0 #<\n",
    "            PU[i][i+1] = self.w/4.0 #>\n",
    "            PU[i][i] = self.w/4.0 #!\n",
    "            PD[i][i-10] = self.w/4.0\n",
    "            PD[i][i-1] = self.w/4.0\n",
    "            PD[i][i+1] = self.w/4.0\n",
    "            PD[i][i] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i-10] = self.w/4.0\n",
    "            PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "            PL[i][i+1] = self.w/4.0\n",
    "            PL[i][i] = self.w/4.0\n",
    "            PR[i][i-10] = self.w/4.0\n",
    "            PR[i][i-1] = self.w/4.0\n",
    "            PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "            PR[i][i] = self.w/4.0\n",
    "        \n",
    "            \n",
    "        # for agent at corner states\n",
    "        PU[0][0] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PU[0][1] = self.w/4.0\n",
    "        PU[0][10] = self.w/4.0\n",
    "        PD[0][0] = self.w/4.0 + self.w/4.0\n",
    "        PD[0][1] = self.w/4.0\n",
    "        PD[0][10] = 1 - self.w + self.w/4.0 \n",
    "        PL[0][0] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PL[0][1] = self.w/4.0\n",
    "        PL[0][10] = self.w/4.0\n",
    "        PR[0][0] = self.w/4.0 + self.w/4.0\n",
    "        PR[0][1] = 1 - self.w + self.w/4.0\n",
    "        PR[0][10] = self.w/4.0\n",
    "                \n",
    "        PU[9][9] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PU[9][8] = self.w/4.0\n",
    "        PU[9][19] = self.w/4.0        \n",
    "        PD[9][9] = self.w/4.0 + self.w/4.0\n",
    "        PD[9][8] = self.w/4.0\n",
    "        PD[9][19] = 1 - self.w + self.w/4.0\n",
    "        PL[9][9] = self.w/4.0 + self.w/4.0\n",
    "        PL[9][8] = 1 - self.w + self.w/4.0 \n",
    "        PL[9][19] = self.w/4.0\n",
    "        PR[9][9] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PR[9][8]= self.w/4.0\n",
    "        PR[9][19] = self.w/4.0\n",
    "           \n",
    "        PU[90][90] = self.w/4.0 + self.w/4.0\n",
    "        PU[90][80] = 1 - self.w + self.w/4.0 \n",
    "        PU[90][91] = self.w/4.0            \n",
    "        PD[90][90] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PD[90][80] = self.w/4.0\n",
    "        PD[90][91] = self.w/4.0           \n",
    "        PL[90][90] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PL[90][80] = self.w/4.0\n",
    "        PL[90][91] = self.w/4.0           \n",
    "        PR[90][90] = self.w/4.0 + self.w/4.0\n",
    "        PR[90][80]= self.w/4.0\n",
    "        PR[90][91] = 1 - self.w + self.w/4.0 \n",
    "        \n",
    "        PU[99][99] = self.w/4.0 + self.w/4.0\n",
    "        PU[99][89] = 1 - self.w + self.w/4.0 \n",
    "        PU[99][98] = self.w/4.0         \n",
    "        PD[99][99] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PD[99][89] = self.w/4.0\n",
    "        PD[99][98] = self.w/4.0           \n",
    "        PL[99][99] = self.w/4.0 + self.w/4.0  \n",
    "        PL[99][89] = self.w/4.0\n",
    "        PL[99][98] = 1 - self.w + self.w/4.0          \n",
    "        PR[99][99] = 1 - self.w + self.w/4.0 + self.w/4.0\n",
    "        PR[99][89]= self.w/4.0\n",
    "        PR[99][98] = self.w/4.0\n",
    "        \n",
    "        # for other states\n",
    "        for m in range(1,9):\n",
    "            for n in range(1,9):\n",
    "                i = m*10 + n\n",
    "                PU[i][i-10] = 1 - self.w + self.w/4.0 #^\n",
    "                PU[i][i-1] = self.w/4.0 #<\n",
    "                PU[i][i+1] = self.w/4.0 #>\n",
    "                PU[i][i+10] = self.w/4.0 #!\n",
    "                PD[i][i-10] = self.w/4.0\n",
    "                PD[i][i-1] = self.w/4.0\n",
    "                PD[i][i+1] = self.w/4.0\n",
    "                PD[i][i+10] = 1 - self.w + self.w/4.0\n",
    "                PL[i][i-10] = self.w/4.0\n",
    "                PL[i][i-1] = 1 - self.w + self.w/4.0\n",
    "                PL[i][i+1] = self.w/4.0\n",
    "                PL[i][i+10] = self.w/4.0\n",
    "                PR[i][i-10] = self.w/4.0\n",
    "                PR[i][i-1] = self.w/4.0\n",
    "                PR[i][i+1] = 1 - self.w + self.w/4.0\n",
    "                PR[i][i+10] = self.w/4.0\n",
    "        \n",
    "        self.PU = PU\n",
    "        self.PD = PD\n",
    "        self.PL = PL\n",
    "        self.PR = PR\n",
    "        \n",
    "        #reward function\n",
    "        \n",
    "        reward = [0]*100\n",
    "        reward[14] = -100\n",
    "        reward[15] = -100\n",
    "        reward[16] = -100\n",
    "        reward[24] = -100\n",
    "        reward[26] = -100\n",
    "        reward[34] = -100\n",
    "        reward[36] = -100\n",
    "        reward[37] = -100\n",
    "        reward[38] = -100\n",
    "        reward[44] = -100\n",
    "        reward[48] = -100\n",
    "        reward[54] = -100\n",
    "        reward[58] = -100\n",
    "        reward[64] = -100\n",
    "        reward[68] = -100\n",
    "        reward[76] = -100\n",
    "        reward[77] = -100\n",
    "        reward[78] = -100\n",
    "        reward[86] = -100\n",
    "        reward[-1] = 10\n",
    "        \"\"\"\n",
    "        reward = [0]*100\n",
    "        reward[99] = 1.0\n",
    "        \"\"\"\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(w =0.1,disc = gamma) \n",
    "\n",
    "# action 0 is 'up'; action 1 is 'down'; action 2 is 'left'; action 3 is 'right'.\n",
    "\n",
    "P2 = [[1, 1, 1, 2, 2, 3, 3, 3, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 0, 3, 3, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 3, 3, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 1, 0, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 1, 1, 3, 1],\n",
    "      [1, 1, 1, 2, 2, 1, 1, 2, 3, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 2, 2, 3, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 2, 1, 1, 1],\n",
    "      [3, 3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [3, 3, 3, 3, 3, 3, 3, 3, 3, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    " # optimal function# optima \n",
    "def value_iteration(env,R):\n",
    "    #initialization\n",
    "    V = env.S.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            V[i][j] = 0\n",
    "    change = math.inf\n",
    "    \n",
    "    \n",
    "    #estimation\n",
    "    while(change > 0.01):\n",
    "        change = 0\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                v = V[i][j]\n",
    "                k = i * 10 + j\n",
    "                sumu = 0\n",
    "                sumd = 0\n",
    "                suml = 0\n",
    "                sumr = 0\n",
    "                c = 0\n",
    "                for m in env.PU[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PD[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PL[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PR[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                V[i][j] = max(sumd, sumu, sumr, suml)\n",
    "\n",
    "                change = max(change, abs(v - V[i][j]))\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_modification(env,R,epi):\n",
    "    #initialization\n",
    "    V = env.S.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            V[i][j] = 0\n",
    "    change = math.inf\n",
    "    \n",
    "    \n",
    "    #estimation\n",
    "    while(change > 0.01):\n",
    "        change = 0\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                v = V[i][j]\n",
    "                k = i * 10 + j\n",
    "                sumu = 0\n",
    "                sumd = 0\n",
    "                suml = 0\n",
    "                sumr = 0\n",
    "                c = 0\n",
    "                for m in env.PU[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PD[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PL[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                c = 0\n",
    "                for m in env.PR[k]:\n",
    "                    if(m != 0):\n",
    "                        t = c//10\n",
    "                        z = c - t * 10\n",
    "                        sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                    c = c + 1\n",
    "                # epislon-greedy algorithm\n",
    "                rnd = random.random() \n",
    "                if rnd < epi: \n",
    "                    V[i][j] = max(sumd, sumu, sumr, suml)\n",
    "                else:\n",
    "                    action = [0,1,2,3]\n",
    "                    values = [sumd, sumu, sumr, suml]\n",
    "                    action_taken = random.sample(action,1)\n",
    "                    #print(action_taken)\n",
    "                    V[i][j] = values[action_taken[0]]\n",
    "                change = max(change, abs(v - V[i][j]))\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(env,R):\n",
    "    V=value_iteration(env,R)\n",
    "    arrow=[[0 for x in range(10)] for y in range(10)]\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            v = V[i][j]\n",
    "            k = i * 10 + j\n",
    "            sumu = 0\n",
    "            sumd = 0\n",
    "            suml = 0\n",
    "            sumr = 0\n",
    "            c = 0\n",
    "            maxnum =0\n",
    "            for m in env.PU[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            maxnum=sumu\n",
    "            c = 0\n",
    "            for m in env.PD[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            if(sumd>maxnum):\n",
    "                arrow[i][j]=1\n",
    "                maxnum=sumd\n",
    "            \n",
    "            c = 0\n",
    "            for m in env.PL[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            c = 0\n",
    "            if(suml>maxnum):\n",
    "                arrow[i][j]=2\n",
    "                maxnum=suml\n",
    "            \n",
    "            for m in env.PR[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            if(sumr>maxnum):\n",
    "                arrow[i][j]=3\n",
    "                maxnum=sumr \n",
    "    #print(arrow)\n",
    "    return arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_border(state, action):\n",
    "    if (state%10 == 0):\n",
    "        return (action == 2)\n",
    "    elif (state%10 == 9):\n",
    "        return (action == 3)\n",
    "    elif (state >= 0 and state <= 9):\n",
    "        return (action == 0)\n",
    "    elif (state < 99 and state >= 90):\n",
    "        return (action == 1)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy_m(env,R,V):\n",
    "    arrow=[[0 for x in range(10)] for y in range(10)]\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            v = V[i][j]\n",
    "            k = i * 10 + j\n",
    "            sumu = 0\n",
    "            sumd = 0\n",
    "            suml = 0\n",
    "            sumr = 0\n",
    "            c = 0\n",
    "            maxnum =0\n",
    "            for m in env.PU[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumu = sumu + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "\n",
    "            c = 0\n",
    "            for m in env.PD[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumd = sumd + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            \n",
    "            c = 0\n",
    "            for m in env.PL[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    suml = suml + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "            c = 0\n",
    "            \n",
    "            for m in env.PR[k]:\n",
    "                if(m != 0):\n",
    "                    t = c//10\n",
    "                    z = c - t * 10\n",
    "                    sumr = sumr + m * (R[c] + env.disc * V[t][z])\n",
    "                c = c + 1\n",
    "\n",
    "            action_values = [sumu,sumd,suml,sumr]\n",
    "            action = np.argmax(action_values)\n",
    "            if out_of_border(k,action):\n",
    "                #print (k)\n",
    "                #print (action)\n",
    "                #print (action_values)\n",
    "                action_values[action] = -9999\n",
    "                action = np.argmax(action_values)\n",
    "            arrow[i][j] = action\n",
    "            \n",
    "    return arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = [0]*100\n",
    "reward[14] = -100\n",
    "reward[15] = -100\n",
    "reward[16] = -100\n",
    "reward[24] = -100\n",
    "reward[26] = -100\n",
    "reward[34] = -100\n",
    "reward[36] = -100\n",
    "reward[37] = -100\n",
    "reward[38] = -100\n",
    "reward[44] = -100\n",
    "reward[48] = -100\n",
    "reward[54] = -100\n",
    "reward[58] = -100\n",
    "reward[64] = -100\n",
    "reward[68] = -100\n",
    "reward[76] = -100\n",
    "reward[77] = -100\n",
    "reward[78] = -100\n",
    "reward[86] = -100\n",
    "reward[-1] = 10\n",
    "\n",
    "R2 = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "def irl(state_num, PU, PD, PL, PR , lambda_num, discount, Rmax, policy):\n",
    "    \"\"\"\n",
    "    - state_num: number of state = 100;\n",
    "    - PU, PD, PL, PR: four transition probability matrix 100*100;\n",
    "    - lambda_num: adjustable pernalty coefficient;\n",
    "    - discount: disc;\n",
    "    - Rmax: max value in T_rewardf;\n",
    "    - policy: policy function 10*10, values should be in [0, 1, 2, 3];\n",
    "    \n",
    "     should return the estimated reward function of ground truth reward function 1 or 2.\n",
    "    \n",
    "    \"\"\"\n",
    "    # change reward function dimension to 100*1\n",
    "    policy = np.reshape(policy,(100,1))\n",
    "    #print(policy[1])\n",
    "    \n",
    "    #action set\n",
    "    A = [0, 1, 2, 3]\n",
    "    \n",
    "    # transfer four transition probability matrix to a three dimension probability tensor \n",
    "    P = [[[0]*100]*100]*4\n",
    "    P[0] = PU\n",
    "    P[1] = PD\n",
    "    P[2] = PL\n",
    "    P[3] = PR\n",
    "    #P = np.asarray(P)\n",
    "    #print(P[3][5])\n",
    "    \n",
    "    # a handle for numtiplication in P.\n",
    "    def multiply(state, other_action):\n",
    "        #print(state)\n",
    "        #policy_action = policy[state][0]\n",
    "        policy_action = int(policy[state])\n",
    "        mul = - np.dot(P[policy_action][state] - P[other_action][state]\n",
    "        ,inv(np.eye(state_num) - discount * P[policy_action]))\n",
    "        #print(mul)\n",
    "        return mul\n",
    "    \n",
    "    # D inclusing following parts: [D00 D01 D02]\n",
    "    #                              [D10 D11 D12]\n",
    "    #                              [D20 D21 D22]\n",
    "    #                              [D30 D31 D32]\n",
    "    #                              [D40 D41 D42]\n",
    "    #                              [D50 D51 D52]\n",
    "    \n",
    "    #D00 (300*100)\n",
    "    D00 = np.vstack([multiply(s, a)] for s in range(100) for a in [element for element in A if element!=policy[s][0]])\n",
    "    D01 = np.vstack([np.eye(1, 100, s) for s in range(100) for a in [element for element in A if element!=policy[s][0]]])\n",
    "    D02 = np.zeros((300, 100))\n",
    "    #D03 = np.zeros((300, 100))\n",
    "    \n",
    "    D0 = np.hstack((D00, D01, D02))\n",
    "    \n",
    "    \n",
    "    D10 = np.vstack([multiply(s, a)] for s in range(100) for a in [element for element in A if element!=policy[s][0]])    \n",
    "    D11 = np.zeros((300, 100))\n",
    "    D12 = np.zeros((300, 100))\n",
    "    #D13 = np.zeros((300, 100))\n",
    "    \n",
    "    D1 = np.hstack((D10, D11, D12))\n",
    "    \n",
    "    D20 = np.eye(100 , 100)\n",
    "    D21 = np.zeros((100, 100))\n",
    "    D22 = - np.eye(100 , 100)\n",
    "    #D23 = np.zeros((100, 100))\n",
    "      \n",
    "    D2 = np.hstack((D20, D21, D22))\n",
    "    \n",
    "    D30 = - np.eye(100 , 100)\n",
    "    D31 = np.zeros((100, 100))\n",
    "    D32 = - np.eye(100 , 100)\n",
    "    #D33 = np.zeros((100, 100))\n",
    "      \n",
    "    D3 = np.hstack((D30, D31, D32))\n",
    "    \n",
    "    D40 = np.eye(100 , 100)\n",
    "    D41 = np.zeros((100, 100))\n",
    "    D42 = np.zeros((100, 100))\n",
    "    #D43 = - Rmax * np.eye(100 , 100)\n",
    "      \n",
    "    D4 = np.hstack((D40, D41, D42))\n",
    "        \n",
    "    D50 = - np.eye(100 , 100)\n",
    "    D51 = np.zeros((100, 100))\n",
    "    D52 = np.zeros((100, 100))\n",
    "    #D53 = - Rmax * np.eye(100 , 100)\n",
    "      \n",
    "    D5 = np.hstack((D50, D51, D52))  \n",
    "        \n",
    "    D = np.vstack((D0, D1, D2, D3, D4, D5))\n",
    "    #print(D.shape)\n",
    "    CT = -np.hstack((np.zeros(100), np.ones(100), -lambda_num * np.ones(100)))\n",
    "    \n",
    "    b = np.zeros((800, 1))\n",
    "    bmax = Rmax * np.ones(200)\n",
    "    b = np.append(b, bmax)\n",
    "    #print(b.shape)\n",
    "    \n",
    "    D = matrix(D)\n",
    "    CT = matrix(CT)\n",
    "    b = matrix(b)\n",
    "    sol=solvers.lp(CT,D,b)\n",
    "    r = np.asarray(sol[\"x\"][:100], dtype=np.double)\n",
    "    #print(len(sol['x']))\n",
    "    return r.reshape((100,))\n",
    "    #return sol['x'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  0.0000e+00 -2.7956e+04  3e+04  2e-02  2e+01  1e+00\n",
      " 1:  2.3437e+01 -2.1139e+04  2e+04  2e-02  2e+01  2e+00\n",
      " 2:  3.9067e+01 -1.5687e+04  2e+04  1e-02  1e+01  3e+00\n",
      " 3: -4.5156e+01 -7.1241e+03  9e+03  6e-03  5e+00  3e+00\n",
      " 4: -3.8990e+02 -4.0974e+03  5e+03  3e-03  3e+00  3e+00\n",
      " 5: -1.6667e+03 -5.6357e+03  9e+03  3e-03  3e+00  5e+00\n",
      " 6: -2.1817e+03 -6.1237e+03  1e+04  3e-03  3e+00  6e+00\n",
      " 7: -3.2758e+03 -5.7861e+03  8e+03  2e-03  2e+00  5e+00\n",
      " 8: -4.2184e+03 -5.7268e+03  6e+03  1e-03  1e+00  4e+00\n",
      " 9: -4.2513e+03 -5.5006e+03  5e+03  1e-03  9e-01  4e+00\n",
      "10: -4.9769e+03 -5.4492e+03  2e+03  4e-04  3e-01  2e+00\n",
      "11: -5.1688e+03 -5.4469e+03  1e+03  2e-04  2e-01  1e+00\n",
      "12: -5.2332e+03 -5.4506e+03  1e+03  2e-04  2e-01  9e-01\n",
      "13: -5.3787e+03 -5.4837e+03  5e+02  8e-05  8e-02  5e-01\n",
      "14: -5.3813e+03 -5.4815e+03  5e+02  8e-05  7e-02  4e-01\n",
      "15: -5.4715e+03 -5.5071e+03  2e+02  3e-05  3e-02  2e-01\n",
      "16: -5.5074e+03 -5.5165e+03  5e+01  7e-06  7e-03  4e-02\n",
      "17: -5.5102e+03 -5.5167e+03  3e+01  5e-06  5e-03  3e-02\n",
      "18: -5.5169e+03 -5.5185e+03  8e+00  1e-06  1e-03  8e-03\n",
      "19: -5.5184e+03 -5.5189e+03  2e+00  4e-07  3e-04  2e-03\n",
      "20: -5.5189e+03 -5.5190e+03  4e-01  6e-08  6e-05  4e-04\n",
      "21: -5.5190e+03 -5.5190e+03  5e-02  8e-09  7e-06  5e-05\n",
      "22: -5.5190e+03 -5.5190e+03  6e-04  1e-10  9e-08  6e-07\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimal_policy() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f8bbb213d421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mirl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPR\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0marrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: optimal_policy() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def s_range(start, end, step):\n",
    "    while start <= end:\n",
    "        yield start\n",
    "        start += step\n",
    "\n",
    "accuracy = []\n",
    "for i in s_range(0, 5, 0.01):\n",
    "    print(i)\n",
    "    R = irl(100, env.PU, env.PD, env.PL, env.PR , i, gamma, 100, P2) \n",
    "    arrow = optimal_policy(Environment(w=0.1,disc=gamma),R,0)\n",
    "    P = np.asarray(P2)\n",
    "    acc = np.sum(arrow == P)\n",
    "    accuracy = np.append(accuracy, acc/100)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
